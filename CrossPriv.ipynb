{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossPriv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/CrossPriv/blob/master/CrossPriv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoq3s2Yzd15t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install syft==0.2.6 --quiet \n",
        "# !pip install pydicom --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6bD7JbBq0D6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05396923-cec7-427d-bb89-9dcd9fbfdddb"
      },
      "source": [
        "import os \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import syft as sy\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time\n",
        "import csv \n",
        "import copy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from syft.frameworks.torch.fl import utils\n",
        "from syft.workers.websocket_client import WebsocketClientWorker\n",
        "import pydicom \n",
        "import pandas as pd \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwuyc_DerKUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeNonFedData():\n",
        "  dcm_path=os.listdir('/content/drive/My Drive/Fed_Covid/minibatch/')\n",
        "  dcm_data={}\n",
        "  alpha = 1.5 \n",
        "  beta = 0\n",
        "  labels=[]\n",
        "  pid=[]\n",
        "  dicom=[]\n",
        "  label=[]\n",
        "  for file in dcm_path:\n",
        "    name = '/content/drive/My Drive/Fed_Covid/minibatch/' + file\n",
        "    temp = pydicom.dcmread(name)\n",
        "    image = temp.pixel_array\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    adjusted = cv2.resize(image,(128,128))\n",
        "    dcm_data[file]=adjusted  \n",
        "  with open('/content/drive/My Drive/Fed_Covid/stage_2_train_labels.csv','r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "      labels.append(row)  \n",
        "  scaler = MinMaxScaler()\n",
        "  cid = 0\n",
        "  for PID in labels:\n",
        "    for key in dcm_data:\n",
        "      if(key[:-4]==PID[0]):\n",
        "        l=[]\n",
        "        for val in dcm_data[key]:\n",
        "          l.append(scaler.fit_transform(val))\n",
        "        l = np.reshape(l,(3,128,128))\n",
        "        dicom.append(l)\n",
        "        label.append(int(PID[5]))\n",
        "  \n",
        "  return dicom,label\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuoufluUrqEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dicom, label = makeNonFedData()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOeS4TuIgogO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 8\n",
        "        self.test_batch_size = 4\n",
        "        self.epochs = 25\n",
        "        self.lr = 0.001\n",
        "        self.log_interval = 10\n",
        "args = Arguments()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td7w-9SHFgaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dicom,label,test_size=0.3)\n",
        "test_df = pd.DataFrame()\n",
        "test_df['features']=x_test\n",
        "test_df['labels']=y_test\n",
        "x_train= np.array(x_train)\n",
        "y_train= np.array(y_train)\n",
        "x_test= np.array(x_test)\n",
        "y_test= np.array(y_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8KCNM1rpsJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class data_maker(Dataset):\n",
        "  def __init__(self, images, labels):\n",
        "        self.data = images \n",
        "        self.targets = labels \n",
        "\n",
        "        self.to_torchtensor()\n",
        "        \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def to_torchtensor(self):      \n",
        "      self.data=torch.from_numpy(self.data)\n",
        "      self.labels=torch.from_numpy(self.targets)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "      sample=self.data[idx]\n",
        "      target=self.targets[idx]\n",
        "      return sample,target"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HjuI6IJnOo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(torch)\n",
        "hospital = sy.VirtualWorker(hook, id=\"hospital\")  \n",
        "clinic = sy.VirtualWorker(hook, id=\"clinic\")  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGOtNPrLcCXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_data = data_maker(x_train,y_train).federate((hospital,clinic))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVmlJAfUcCUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader(federated_train_data,batch_size=args.batch_size)\n",
        "test_data = data_maker(x_test,y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpiILdmui8ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()  \n",
        "        self.conv1 = nn.Conv2d(3,32, kernel_size=8, stride = 2)  \n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=4,stride=2,padding=0)  \n",
        "        self.conv2 = nn.Conv2d(32, 64, 8)\n",
        "        self.pool2 = nn.MaxPool2d(8,8,padding=0)\n",
        "\n",
        "        # Linear Layers \n",
        "        \n",
        "        self.fc1 = nn.Linear(64*2*2, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # print(x.shape)\n",
        "        x = self.pool1(x)\n",
        "        # print(x.shape)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # print(x.shape)\n",
        "        x = self.pool2(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1,64*2*2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        " \n",
        "        return F.log_softmax(x,dim=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6NZuEi-lmCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hospital_model = Net().double()\n",
        "clinic_model = Net().double()\n",
        "hospital_optimizer = optim.Adam(hospital_model.parameters(), lr=0.0001)\n",
        "clinic_optimizer = optim.Adam(clinic_model.parameters(), lr=0.0001)\n",
        "models = [hospital_model, clinic_model]\n",
        "optimizers = [hospital_optimizer, clinic_optimizer]\n",
        "model = Net().double()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xTEMi9Alujt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compute_nodes = [hospital, clinic]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeC0JqCE2oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): \n",
        "        model.send(data.location) \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "    \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Ei8_ogE2mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model,test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    #criterion = F.nll_loss()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHQk6NAVE2j9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbdd1bbc-7c53-41bc-af1e-8a8185bd5393"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model,  test_loader)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/304 (0%)]\tLoss: 0.672762\n",
            "Train Epoch: 1 [80/304 (26%)]\tLoss: 0.669482\n",
            "Train Epoch: 1 [160/304 (53%)]\tLoss: 0.664675\n",
            "Train Epoch: 1 [240/304 (79%)]\tLoss: 0.661959\n",
            "\n",
            "Test set: Average loss: 0.6497, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 2 [0/304 (0%)]\tLoss: 0.657960\n",
            "Train Epoch: 2 [80/304 (26%)]\tLoss: 0.657128\n",
            "Train Epoch: 2 [160/304 (53%)]\tLoss: 0.652800\n",
            "Train Epoch: 2 [240/304 (79%)]\tLoss: 0.650527\n",
            "\n",
            "Test set: Average loss: 0.6352, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 3 [0/304 (0%)]\tLoss: 0.646872\n",
            "Train Epoch: 3 [80/304 (26%)]\tLoss: 0.646597\n",
            "Train Epoch: 3 [160/304 (53%)]\tLoss: 0.642013\n",
            "Train Epoch: 3 [240/304 (79%)]\tLoss: 0.639826\n",
            "\n",
            "Test set: Average loss: 0.6211, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 4 [0/304 (0%)]\tLoss: 0.636124\n",
            "Train Epoch: 4 [80/304 (26%)]\tLoss: 0.636909\n",
            "Train Epoch: 4 [160/304 (53%)]\tLoss: 0.632105\n",
            "Train Epoch: 4 [240/304 (79%)]\tLoss: 0.629956\n",
            "\n",
            "Test set: Average loss: 0.6081, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 5 [0/304 (0%)]\tLoss: 0.626157\n",
            "Train Epoch: 5 [80/304 (26%)]\tLoss: 0.627969\n",
            "Train Epoch: 5 [160/304 (53%)]\tLoss: 0.622963\n",
            "Train Epoch: 5 [240/304 (79%)]\tLoss: 0.620626\n",
            "\n",
            "Test set: Average loss: 0.5952, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 6 [0/304 (0%)]\tLoss: 0.616581\n",
            "Train Epoch: 6 [80/304 (26%)]\tLoss: 0.619271\n",
            "Train Epoch: 6 [160/304 (53%)]\tLoss: 0.613784\n",
            "Train Epoch: 6 [240/304 (79%)]\tLoss: 0.611088\n",
            "\n",
            "Test set: Average loss: 0.5819, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 7 [0/304 (0%)]\tLoss: 0.606888\n",
            "Train Epoch: 7 [80/304 (26%)]\tLoss: 0.610511\n",
            "Train Epoch: 7 [160/304 (53%)]\tLoss: 0.604714\n",
            "Train Epoch: 7 [240/304 (79%)]\tLoss: 0.601597\n",
            "\n",
            "Test set: Average loss: 0.5682, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 8 [0/304 (0%)]\tLoss: 0.597221\n",
            "Train Epoch: 8 [80/304 (26%)]\tLoss: 0.601985\n",
            "Train Epoch: 8 [160/304 (53%)]\tLoss: 0.595944\n",
            "Train Epoch: 8 [240/304 (79%)]\tLoss: 0.592326\n",
            "\n",
            "Test set: Average loss: 0.5543, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 9 [0/304 (0%)]\tLoss: 0.587754\n",
            "Train Epoch: 9 [80/304 (26%)]\tLoss: 0.593940\n",
            "Train Epoch: 9 [160/304 (53%)]\tLoss: 0.587680\n",
            "Train Epoch: 9 [240/304 (79%)]\tLoss: 0.583535\n",
            "\n",
            "Test set: Average loss: 0.5404, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 10 [0/304 (0%)]\tLoss: 0.578734\n",
            "Train Epoch: 10 [80/304 (26%)]\tLoss: 0.586659\n",
            "Train Epoch: 10 [160/304 (53%)]\tLoss: 0.580334\n",
            "Train Epoch: 10 [240/304 (79%)]\tLoss: 0.575682\n",
            "\n",
            "Test set: Average loss: 0.5271, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 11 [0/304 (0%)]\tLoss: 0.570734\n",
            "Train Epoch: 11 [80/304 (26%)]\tLoss: 0.580519\n",
            "Train Epoch: 11 [160/304 (53%)]\tLoss: 0.574572\n",
            "Train Epoch: 11 [240/304 (79%)]\tLoss: 0.569258\n",
            "\n",
            "Test set: Average loss: 0.5150, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 12 [0/304 (0%)]\tLoss: 0.564479\n",
            "Train Epoch: 12 [80/304 (26%)]\tLoss: 0.576071\n",
            "Train Epoch: 12 [160/304 (53%)]\tLoss: 0.570772\n",
            "Train Epoch: 12 [240/304 (79%)]\tLoss: 0.564709\n",
            "\n",
            "Test set: Average loss: 0.5049, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 13 [0/304 (0%)]\tLoss: 0.560215\n",
            "Train Epoch: 13 [80/304 (26%)]\tLoss: 0.573587\n",
            "Train Epoch: 13 [160/304 (53%)]\tLoss: 0.568900\n",
            "Train Epoch: 13 [240/304 (79%)]\tLoss: 0.562278\n",
            "\n",
            "Test set: Average loss: 0.4970, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 14 [0/304 (0%)]\tLoss: 0.558117\n",
            "Train Epoch: 14 [80/304 (26%)]\tLoss: 0.572854\n",
            "Train Epoch: 14 [160/304 (53%)]\tLoss: 0.568795\n",
            "Train Epoch: 14 [240/304 (79%)]\tLoss: 0.561559\n",
            "\n",
            "Test set: Average loss: 0.4914, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 15 [0/304 (0%)]\tLoss: 0.557684\n",
            "Train Epoch: 15 [80/304 (26%)]\tLoss: 0.573374\n",
            "Train Epoch: 15 [160/304 (53%)]\tLoss: 0.569860\n",
            "Train Epoch: 15 [240/304 (79%)]\tLoss: 0.561962\n",
            "\n",
            "Test set: Average loss: 0.4877, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 16 [0/304 (0%)]\tLoss: 0.558271\n",
            "Train Epoch: 16 [80/304 (26%)]\tLoss: 0.574587\n",
            "Train Epoch: 16 [160/304 (53%)]\tLoss: 0.571398\n",
            "Train Epoch: 16 [240/304 (79%)]\tLoss: 0.562929\n",
            "\n",
            "Test set: Average loss: 0.4852, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 17 [0/304 (0%)]\tLoss: 0.559309\n",
            "Train Epoch: 17 [80/304 (26%)]\tLoss: 0.575968\n",
            "Train Epoch: 17 [160/304 (53%)]\tLoss: 0.573004\n",
            "Train Epoch: 17 [240/304 (79%)]\tLoss: 0.564037\n",
            "\n",
            "Test set: Average loss: 0.4837, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 18 [0/304 (0%)]\tLoss: 0.560434\n",
            "Train Epoch: 18 [80/304 (26%)]\tLoss: 0.577247\n",
            "Train Epoch: 18 [160/304 (53%)]\tLoss: 0.574422\n",
            "Train Epoch: 18 [240/304 (79%)]\tLoss: 0.565046\n",
            "\n",
            "Test set: Average loss: 0.4828, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 19 [0/304 (0%)]\tLoss: 0.561431\n",
            "Train Epoch: 19 [80/304 (26%)]\tLoss: 0.578308\n",
            "Train Epoch: 19 [160/304 (53%)]\tLoss: 0.575553\n",
            "Train Epoch: 19 [240/304 (79%)]\tLoss: 0.565859\n",
            "\n",
            "Test set: Average loss: 0.4822, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 20 [0/304 (0%)]\tLoss: 0.562220\n",
            "Train Epoch: 20 [80/304 (26%)]\tLoss: 0.579079\n",
            "Train Epoch: 20 [160/304 (53%)]\tLoss: 0.576354\n",
            "Train Epoch: 20 [240/304 (79%)]\tLoss: 0.566433\n",
            "\n",
            "Test set: Average loss: 0.4818, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 21 [0/304 (0%)]\tLoss: 0.562816\n",
            "Train Epoch: 21 [80/304 (26%)]\tLoss: 0.579551\n",
            "Train Epoch: 21 [160/304 (53%)]\tLoss: 0.576876\n",
            "Train Epoch: 21 [240/304 (79%)]\tLoss: 0.566821\n",
            "\n",
            "Test set: Average loss: 0.4815, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 22 [0/304 (0%)]\tLoss: 0.563234\n",
            "Train Epoch: 22 [80/304 (26%)]\tLoss: 0.579884\n",
            "Train Epoch: 22 [160/304 (53%)]\tLoss: 0.577222\n",
            "Train Epoch: 22 [240/304 (79%)]\tLoss: 0.567070\n",
            "\n",
            "Test set: Average loss: 0.4813, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 23 [0/304 (0%)]\tLoss: 0.563482\n",
            "Train Epoch: 23 [80/304 (26%)]\tLoss: 0.580059\n",
            "Train Epoch: 23 [160/304 (53%)]\tLoss: 0.577401\n",
            "Train Epoch: 23 [240/304 (79%)]\tLoss: 0.567166\n",
            "\n",
            "Test set: Average loss: 0.4812, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 24 [0/304 (0%)]\tLoss: 0.563623\n",
            "Train Epoch: 24 [80/304 (26%)]\tLoss: 0.580130\n",
            "Train Epoch: 24 [160/304 (53%)]\tLoss: 0.577483\n",
            "Train Epoch: 24 [240/304 (79%)]\tLoss: 0.567207\n",
            "\n",
            "Test set: Average loss: 0.4810, Accuracy: 105/129 (81%)\n",
            "\n",
            "Train Epoch: 25 [0/304 (0%)]\tLoss: 0.563692\n",
            "Train Epoch: 25 [80/304 (26%)]\tLoss: 0.580178\n",
            "Train Epoch: 25 [160/304 (53%)]\tLoss: 0.577511\n",
            "Train Epoch: 25 [240/304 (79%)]\tLoss: 0.567230\n",
            "\n",
            "Test set: Average loss: 0.4810, Accuracy: 105/129 (81%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}